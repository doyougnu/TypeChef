* Figuring out the sat interface
** Frontend
*** isSatisfiable method called from featureexpr.FeatureExpr
** FeatureExpr
** Solver Interface
*** in "featureexpr/sat/SatSolver"

*** Important Question to answer

     - 1 [ ] find out when the caching is done; is it done before or after queries?

       - [ ] we know caching is hit before an ~isSat~ call, but we don't know if
         caching is hit before or after queries

     - 2 [X] As a baseline, how is TypeChef handling the feature model? What is the
       interface to the solver here, is the FM added only once and then
       incremental solving occurs for subsequent queries

     - 3 [ ] Which of the queries come from the parsing and which from the type checking?

     - 4 [ ] Are there any advantages in doing the Parsing, Type Checking, and data
       flow analysis at the same time

**** 1 Caching hit notes
     - Note taken on [2020-09-17 Thu 23:18] \\
       line 60 of ~CTypeSystemFrontend.scala~ is a good starting point which shows the
       feature model and abstract syntax tree
     - Note taken on [2020-09-17 Thu 21:58] \\
       We found the connection to Sat4j in
       /store/Research/TypeChef/FeatureExprLib/src/main/scala/de/fosd/typechef/featureexpr/sat line 180
       we have annotated important lines with ~[VSAT]~

**** 2 FM handling
     - Note taken on [2020-09-17 Thu 22:53] \\
       FMs are added at least once, and then cached into ~SatSolverImpl~ but we do not
       know if type chef is doing incremental solving with queries yet


*** <2020-09-21 Mon>

**** 3 Where is the feature model handled and going?

     - Note taken on [2020-09-21 Mon 23:00] \\
       In SatSolver(), when a feature model is not in the cache, TypeChef creates a
       SatSolverImpl instance which holds the ~sat4j~ solver and is immediately fed the
       feature model. The feature model is passed in as an argument to the
       SatSolverImpl contructor. The bottom line, the sat4j solver never operates
       without the feature model, and then performs incremental solving after the
       feature model is already loaded.

     - Note taken on [2020-09-21 Mon 22:36] \\
       The sat solver has a sub class called SatSolverImpl. A feature model is
       cached in a map, if the feature model has not been sent, then it is passed
       to SatSolver, which will create a sat solver impl instance, and pass the
       feature model to SatSolverImpl constructor. See line 148 in
       SATFeatureExpr and line 31 in SatSolver:
       #+begin_src
        cacheIsSatisfiable.getOrElseUpdate(f, new SatSolver().isSatisfiable(toCnfEquiSat, f))
        #+end_src

       #+begin_src
        (if (CACHING && (nfm(featureModel) != SATNoFeatureModel))
          SatSolverCache.get(nfm(featureModel))
        else
          new SatSolverImpl(nfm(featureModel),false)).isSatisfiable(exprCNF)
        #+end_src

     - Note taken on [2020-09-21 Mon 21:46] \\
       in Fronted.scala function ~processFile~ the feature model is input
       through the options record

***** Next Step
      Check under which feature model the queries are made, then group queries
      by feature model, and construct choices over the queries.

*** <2020-10-06 Tue>

**** Questions in order of importance
      - Note taken on [2020-10-07 Wed 00:09] \\
        We added several methods to the front end class and the solver class. We
        experienced issues using environment variables and just decided to rely
        on files instead. The system for recording works like this: we define a
        file ~VSAT_MODE~ which holds the mode the present query, then we mutate
        that file using ~vsat_set_env~ to record which mode type chef is in. We
        then use ~vsat_record_query~ to record the queries to their respective
        files, that is lexing queries go to ~SAT_problems_LEXING.txt~ and so on.

      - [X] Jeff and Paul, AI: focus on finding which queries are the Parser
            queries, and which are the Type Checking queries

      - [X] Jeff and Paul, AI: try to figure out if BDDs are created from PCs and
            then cached and used in the solving routine.

        - Note taken on [2020-10-08 Thu 23:16] \\
          We observed no evidence that BDDs were exercised in the busy box
          analysis, however when running the linux analysis bdds were
          exercised immediately

        - Note taken on [2020-10-08 Thu 23:26] \\
          BDDs are only used with "--bdd" flag and they are used to simplify the presences
          conditions ~[ParserMain.scala, line 105]~. This is deactivated for busy box by
          default in the ~./analyzeBusybox.sh~ script. BDDs are used by default for Linux, why?

      - [ ] Jeff and Paul, AI: why are BDDs used to simplify the presence
        conditions only for Linux but not for BusyBox

*** <2020-10-15 Thu>

**** DONE Capturing feature models
     CLOSED: [2020-10-15 Thu 21:17]
     - Note taken on [2020-10-15 Thu 21:22] \\
       I've finished capturing the feature models, this is purely done through
       the environment variable files we've already been using. We keep one that
       counts to serve as a ID for feature models, one to capture the mode, and
       one to capture the path to the right sub-directory for the feature model.
       We keep a directory ~sat_queries~ which stores the queries for each
       feature model. When we observe a query without a feature model we store
       it in ~sat_queries/plain~, queries found with feature models are stored
       in sub directories which indicate the model they were logged in. For
       example, ~sat-queries/0~ will store all the queries that were found for
       the first feature model in separate files indicating their mode. I
       created a new mode ~FEATURE_MODEL~ to indicate the feature model that
       that directory belongs to. To track the feature models we use a hash map
       which associates the feature models to integers which serve as the UUIDs.
       This allows our hacked version to capture queries properly even if they
       appear out of order on the solver side

     - Note taken on [2020-10-15 Thu 16:45] \\
       I (Jeff) found the section of code in the front end that captures the
       feature models. It looks like Type Chef parses the feature models and
       serializes them to files to save time, see line 112 in ~Frontend.scala~.
       The code reuses the satisfiability solver to test if the feature model is
       satisfiable thus we can reuse our hack-y capturing code but must mark the
       new environment. Or in other words, we _were_ capturing the feature
       models but didn't know how to differentiate them from the actual queries.

**** DONE remove hard coded file paths
     CLOSED: [2020-10-15 Thu 21:29]
